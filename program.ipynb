{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2a10d5-4c74-4c55-9593-093636cf1c94",
   "metadata": {},
   "source": [
    "Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e7419e-d284-4942-8111-4ba2f5ee6d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Read an image\n",
    "image = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample.png\")\n",
    "\n",
    "# Check if the image was successfully loaded\n",
    "\n",
    "if image is None:\n",
    "    print('Error: Unable to read the image.')\n",
    "else:\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Display the original and grayscale images\n",
    "    cv2.imshow('Original Image', image)\n",
    "    cv2.imshow('Grayscale Image', gray_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f2f58-39b8-4bad-ac1a-49d718976e00",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43bb5327-fa27-4d1f-a586-944238ff7214",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) :-1: error: (-5:Bad argument) in function 'GaussianBlur'\n> Overload resolution failed:\n>  - GaussianBlur() missing required argument 'sigmaX' (pos 3)\n>  - GaussianBlur() missing required argument 'sigmaX' (pos 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m gray_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Apply Gaussian blur\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m blurred_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mGaussianBlur(gray_image, (\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Display the original and blurred images\u001b[39;00m\n\u001b[0;32m     17\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal Image\u001b[39m\u001b[38;5;124m'\u001b[39m, image)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) :-1: error: (-5:Bad argument) in function 'GaussianBlur'\n> Overload resolution failed:\n>  - GaussianBlur() missing required argument 'sigmaX' (pos 3)\n>  - GaussianBlur() missing required argument 'sigmaX' (pos 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Read an image\n",
    "image = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample.png\")\n",
    "\n",
    "# Check if the image was successfully loaded\n",
    "if image is None:\n",
    "    print('Error: Unable to read the image.')\n",
    "else:\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian blur\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "\n",
    "    # Display the original and blurred images\n",
    "    cv2.imshow('Original Image', image)\n",
    "    cv2.imshow('Blurred Image', blurred_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783a31f0-3e81-448c-8f5a-6270a46003e5",
   "metadata": {},
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c351d01-377d-4a86-97b3-86cd8b3a2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Read an image\n",
    "image = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\")\n",
    "\n",
    "# Check if the image was successfully loaded\n",
    "if image is None:\n",
    "    print('Error: Unable to read the image.')\n",
    "else:\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Canny edge detection\n",
    "    edges = cv2.Canny(gray_image, 100, 200)  # Adjust thresholds as needed\n",
    "\n",
    "    # Display the original and edge-detected images\n",
    "    cv2.imshow('Original Image', image)\n",
    "    cv2.imshow('Edges', edges)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6086eab3-c537-4a0c-b7cd-067dcce89dff",
   "metadata": {},
   "source": [
    "question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e7c8b-ab46-48f3-ab12-385f266af5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read an image\n",
    "image = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\")\n",
    "\n",
    "# Check if the image was successfully loaded\n",
    "if image is None:\n",
    "    print('Error: Unable to read the image.')\n",
    "else:\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Perform binary thresholding (optional)\n",
    "    _, binary_image = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Define kernel for dilation\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "\n",
    "    # Dilate the binary image\n",
    "    dilated_image = cv2.dilate(binary_image, kernel, iterations=1)\n",
    "\n",
    "    # Display the original and dilated images\n",
    "    cv2.imshow('Original Image', image)\n",
    "    cv2.imshow('Dilated Image', dilated_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9542a8d-e04e-4553-b3df-19530a4bb5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13dc10c-1772-48c6-ba5c-d5daa51433a3",
   "metadata": {},
   "source": [
    "Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a40d8-a836-4cb7-9fba-0f48149619e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Read an image\n",
    "image = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample.png\")\n",
    "\n",
    "# Check if the image was successfully loaded\n",
    "if image is None:\n",
    "    print('Error: Unable to read the image.')\n",
    "else:\n",
    "    # Perform erosion operation\n",
    "    kernel = np.ones((5,5), np.uint8)  # Define kernel\n",
    "    eroded_image = cv2.erode(image, kernel, iterations=1)  # Erode the image\n",
    "\n",
    "    # Display the original and eroded images\n",
    "    cv2.imshow('Original Image', image)\n",
    "    cv2.imshow('Eroded Image', eroded_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3072d-c5f0-457e-a855-60d76089bb74",
   "metadata": {},
   "source": [
    "Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346adb57-ca44-4acd-8201-b36eaec5582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "cap = cv2.VideoCapture(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Samplemp4.mp4\")\n",
    "if (cap.isOpened()== False):\n",
    "    print(\"Error opening video file\")\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        cv2.imshow('Frame', frame)\n",
    "        if cv2.waitKey(250) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfded75-6edd-472c-84c9-37e776ce9cb3",
   "metadata": {},
   "source": [
    "Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02a2f22-db5f-4b2e-9b50-46260793395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Initialize the webcam\n",
    "video_capture = cv2.VideoCapture(0)  # 0 for default webcam, change if you have multiple webcams\n",
    "\n",
    "# Set the frame size (optional)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "# Get the frame rate of the video\n",
    "fps = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Create a VideoWriter object to write the output video for slow motion\n",
    "output_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "output_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video_slow = cv2.VideoWriter('output_video_slow.mp4', fourcc, fps / 2, (output_width, output_height))  # Adjusted FPS for slow motion\n",
    "\n",
    "# Create a VideoWriter object to write the output video for fast motion\n",
    "output_video_fast = cv2.VideoWriter('output_video_fast.mp4', fourcc, fps * 2, (output_width, output_height))  # Adjusted FPS for fast motion\n",
    "\n",
    "# Read and process each frame\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply slow motion\n",
    "    output_video_slow.write(frame)\n",
    "    output_video_slow.write(frame)\n",
    "\n",
    "    # Apply fast motion\n",
    "    output_video_fast.write(frame)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "video_capture.release()\n",
    "output_video_slow.release()\n",
    "output_video_fast.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9a101-1561-4883-b359-24ed10950e63",
   "metadata": {},
   "source": [
    "Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4f291-626a-4109-b2df-2d829565ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample.png\")\n",
    "\n",
    "# Get the dimensions of the image\n",
    "height, width = image.shape[:2]\n",
    "\n",
    "# Define the scaling factors for bigger and smaller sizes\n",
    "bigger_scale = 1.5  # Scale factor for bigger size (increase by 50%)\n",
    "smaller_scale = 0.5  # Scale factor for smaller size (decrease by 50%)\n",
    "\n",
    "# Scale the image to its bigger size\n",
    "bigger_image = cv2.resize(image, (int(width * bigger_scale), int(height * bigger_scale)))\n",
    "\n",
    "# Scale the image to its smaller size\n",
    "smaller_image = cv2.resize(image, (int(width * smaller_scale), int(height * smaller_scale)))\n",
    "\n",
    "# Display the original image, bigger image, and smaller image\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Bigger Image', bigger_image)\n",
    "cv2.imshow('Smaller Image', smaller_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a07dabd-f61c-4685-a5a1-edab48a90efc",
   "metadata": {},
   "source": [
    "Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2217a0a7-eaf4-4c14-a0c8-2245fb728ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample.png\")\n",
    "\n",
    "# Get the height and width of the image\n",
    "height, width = image.shape[:2]\n",
    "\n",
    "# Define the angle of rotation in degrees\n",
    "angle_clockwise = 45  # Rotate clockwise by 45 degrees\n",
    "angle_counterclockwise = -45  # Rotate counterclockwise by 45 degrees\n",
    "\n",
    "# Perform clockwise rotation\n",
    "rotated_clockwise = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "# Perform counterclockwise rotation\n",
    "rotated_counterclockwise = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\n",
    "# Display the original image and the rotated images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Rotated Clockwise', rotated_clockwise)\n",
    "cv2.imshow('Rotated Counterclockwise', rotated_counterclockwise)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e762a6-a0de-4ff0-b4b2-0a83b6c05e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd6cfa-8641-4c7e-863b-73d1f569f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb1b60-e3e7-4bbb-b66d-abbbca6cf0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample.png\")\n",
    "\n",
    "# Define the translation (movement) in pixels\n",
    "x_translation = 50  # Move 50 pixels to the right\n",
    "y_translation = 30  # Move 30 pixels down\n",
    "\n",
    "# Get the height and width of the image\n",
    "height, width = image.shape[:2]\n",
    "\n",
    "# Create a blank canvas (image) to paste the original image onto\n",
    "canvas = np.zeros((height + y_translation, width + x_translation, 3), dtype=np.uint8)\n",
    "\n",
    "# Paste the original image onto the canvas at the desired position\n",
    "canvas[y_translation:, x_translation:] = image\n",
    "\n",
    "# Display the original image and the moved image\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Moved Image', canvas)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbf5b5e-0e82-4a00-8cf5-02d07fead233",
   "metadata": {},
   "source": [
    "Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f7610-daae-45ae-8815-af3244420185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample.png\")\n",
    "\n",
    "# Define the transformation matrix\n",
    "# We'll perform a translation and rotation in this example\n",
    "# Translation matrix:\n",
    "# [1 0 Tx]\n",
    "# [0 1 Ty]\n",
    "tx = 50  # Translation in x direction (move right by 50 pixels)\n",
    "ty = 30  # Translation in y direction (move down by 30 pixels)\n",
    "translation_matrix = np.float32([[1, 0, tx],\n",
    "                                 [0, 1, ty]])\n",
    "\n",
    "# Rotation angle in degrees\n",
    "angle = 30\n",
    "\n",
    "# Get the height and width of the image\n",
    "height, width = image.shape[:2]\n",
    "\n",
    "# Define the center of rotation (center of the image)\n",
    "center = (width // 2, height // 2)\n",
    "\n",
    "# Get the rotation matrix\n",
    "rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "# Combine the translation and rotation matrices into a single 3x3 matrix\n",
    "combined_matrix = np.vstack([rotation_matrix, [0, 0, 1]])\n",
    "\n",
    "# Perform affine transformation\n",
    "transformed_image = cv2.warpAffine(image, combined_matrix[:2], (width, height))\n",
    "\n",
    "# Display the original and transformed images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Transformed Image', transformed_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a7740-ae8c-4fd4-a7a3-57466e6c005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread('example_image.jpg')\n",
    "\n",
    "# Define the transformation matrix\n",
    "# Example: scaling by 1.5 in x-direction and 0.8 in y-direction\n",
    "scale_x = 1.5\n",
    "scale_y = 0.8\n",
    "transformation_matrix = np.array([[scale_x, 0, 0],\n",
    "                                  [0, scale_y, 0]])\n",
    "\n",
    "# Apply the affine transformation\n",
    "transformed_image = cv2.warpAffine(image, transformation_matrix, (image.shape[1], image.shape[0]))\n",
    "\n",
    "# Display the original and transformed images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Transformed Image', transformed_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead0dbce-7de6-424a-aa04-016987d01f9e",
   "metadata": {},
   "source": [
    "Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c9ab6-73fa-42fb-bdd9-1091afa5d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample.png\")\n",
    "\n",
    "# Define the four source points (coordinates of a rectangle)\n",
    "source_points = np.float32([[50, 50], [300, 50], [300, 200], [50, 200]])\n",
    "\n",
    "# Define the four destination points (where the corners of the rectangle will be mapped)\n",
    "# For demonstration, let's make a perspective transformation (simulate tilting the rectangle)\n",
    "destination_points = np.float32([[100, 100], [250, 50], [200, 300], [50, 250]])\n",
    "\n",
    "# Compute the perspective transformation matrix\n",
    "perspective_matrix = cv2.getPerspectiveTransform(source_points, destination_points)\n",
    "\n",
    "# Perform perspective transformation\n",
    "transformed_image = cv2.warpPerspective(image, perspective_matrix, (image.shape[1], image.shape[0]))\n",
    "\n",
    "# Display the original and transformed images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Transformed Image', transformed_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61d96c4-542a-4b3b-ad0d-89ec7ca8832e",
   "metadata": {},
   "source": [
    "Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a21fe61-e9b2-47a0-9795-cba26746a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the video\n",
    "video_capture = cv2.VideoCapture(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Samplemp4.mp4\")\n",
    "\n",
    "# Get the width and height of the video frames\n",
    "width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the source points (coordinates of a rectangle)\n",
    "source_points = np.float32([[50, 50], [300, 50], [300, 200], [50, 200]])\n",
    "\n",
    "# Define the destination points (where the corners of the rectangle will be mapped)\n",
    "destination_points = np.float32([[100, 100], [250, 50], [200, 300], [50, 250]])\n",
    "\n",
    "# Compute the perspective transformation matrix\n",
    "perspective_matrix = cv2.getPerspectiveTransform(source_points, destination_points)\n",
    "\n",
    "# Define the codec and create a VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter('output_video.mp4', fourcc, 20.0, (width, height))\n",
    "\n",
    "# Process each frame of the video\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break  # Break the loop if no more frames are available\n",
    "\n",
    "    # Perform perspective transformation on the frame\n",
    "    transformed_frame = cv2.warpPerspective(frame, perspective_matrix, (width, height))\n",
    "\n",
    "    # Write the transformed frame to the output video\n",
    "    output_video.write(transformed_frame)\n",
    "\n",
    "    # Display the transformed frame (optional)\n",
    "    cv2.imshow('Transformed Frame', transformed_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # Break the loop if 'q' key is pressed\n",
    "\n",
    "# Release video capture and writer objects\n",
    "video_capture.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd0c331-becf-4777-a934-e5ae6b908392",
   "metadata": {},
   "source": [
    "Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b478d-2b74-4f7f-b6b7-d0d5b32a9161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the images\n",
    "image1 = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample.png\")\n",
    "image2 = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Samplerotate.png\")\n",
    "\n",
    "# Set the corresponding points in both images\n",
    "pts_image1 = np.array([[50, 50], [200, 50], [200, 200], [50, 200]], dtype=np.float32)\n",
    "pts_image2 = np.array([[10, 100], [200, 50], [300, 300], [100,250]], dtype=np.float32)\n",
    "\n",
    "# Calculate the homography matrix\n",
    "homography_matrix, _ = cv2.findHomography(pts_image1, pts_image2, cv2.RANSAC)\n",
    "\n",
    "# Apply the perspective transformation to image1\n",
    "transformed_image = cv2.warpPerspective(image1, homography_matrix, (image2.shape[1], image2.shape[0]))\n",
    "\n",
    "# Display the images\n",
    "cv2.imshow('Original Image 1', image1)\n",
    "cv2.imshow('Original Image 2', image2)\n",
    "cv2.imshow('Transformed Image', transformed_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209d8b3-1a48-4e69-a2fb-7aa7755cf548",
   "metadata": {},
   "source": [
    "Question 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c055fd0-e051-4cbd-836f-48ca05bca4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "# Load images\n",
    "img1 = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\")\n",
    "img2 = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample.png\")\n",
    "# Define corresponding points\n",
    "pts1 = np.array([[50, 50], [200, 50], [50, 200], [200, 200]])\n",
    "pts2 = np.array([[100, 100], [300, 100], [100, 300], [300, 300]])\n",
    "# Estimate projective transformation matrix using DLT\n",
    "H, _ = cv2.findHomography(pts1, pts2)\n",
    "# Apply projective transformation to img1\n",
    "dst = cv2.warpPerspective(img1, H, (img2.shape[1], img2.shape[0]))\n",
    "# Display images\n",
    "cv2.imshow('img1', img1)\n",
    "cv2.imshow('img2', img2)\n",
    "cv2.imshow('dst', dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c271f0c-e783-4b73-8f96-a9b2d03167d7",
   "metadata": {},
   "source": [
    "Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf6a67-5c2a-469f-aa4f-9b4f20f20612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\") \n",
    "cv2.imshow('Original', img)\n",
    "cv2.waitKey(0)\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "img_blur = cv2.GaussianBlur(img_gray, (3,3), 0) \n",
    "edges = cv2.Canny(img_blur,100,200) # Canny Edge Detection\n",
    "cv2.imshow('Canny Edge Detection', edges)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7854994-0080-4af2-b7c7-d939ceacb7d3",
   "metadata": {},
   "source": [
    "Question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1560639-b72f-4532-bbe4-4d15a266df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\") \n",
    "cv2.imshow('Original', img)\n",
    "cv2.waitKey(0)\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# Blur the image for better edge detection\n",
    "img_blur = cv2.GaussianBlur(img_gray, (3,3), 0) \n",
    "sobelx = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5) # Sobel Edge Detection on the X axis\n",
    "cv2.imshow('Sobel X', sobelx)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e406fb43-c4fb-4aa5-9d99-51ef7b49ead8",
   "metadata": {},
   "source": [
    "Question 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346a757-089f-44f8-b162-5d2d30f1f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the original image\n",
    "img = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\") \n",
    "# Display original image\n",
    "cv2.imshow('Original', img)\n",
    "cv2.waitKey(0)\n",
    "# Convert to graycsale\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# Blur the image for better edge detection\n",
    "img_blur = cv2.GaussianBlur(img_gray, (3,3), 0) \n",
    "# Sobel Edge Detection\n",
    "sobely = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5) # Sobel Edge Detection on the Y axis\n",
    "# Display Sobel Edge Detection Images\n",
    "cv2.imshow('Sobel Y', sobely)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4042c5e-9e48-4a9a-b862-e94b35a9dafc",
   "metadata": {},
   "source": [
    "Question 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d851165a-fd5f-47dc-869d-648caa36b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\") \n",
    "# Display original image\n",
    "cv2.imshow('Original', img)\n",
    "cv2.waitKey(0)\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# Blur the image for better edge detection\n",
    "img_blur = cv2.GaussianBlur(img_gray, (3,3), 0) \n",
    "sobelxy = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=1, dy=1, ksize=5) # Combined X and Y Sobel Edge Detection\n",
    "cv2.imshow('Sobel X Y using Sobel() function', sobelxy)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2362cd-7342-4223-8fc9-135a289f64c7",
   "metadata": {},
   "source": [
    "Question 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12145c84-eb1b-4e6a-9404-5c16ebad9820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "kernel = np.array([[0,1,0], [1,-8,1], [0,1,0]])\n",
    "sharpened = cv2.filter2D(gray, -1, kernel)\n",
    "cv2.imshow('Original', gray)\n",
    "cv2.imshow('Sharpened', sharpened)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c64eab-3c5d-4e88-a57f-58e707200502",
   "metadata": {},
   "source": [
    "Question 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3727a669-8927-4920-948f-3c5f5e4a1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "kernel = np.array([[0,1,0], [1,-4,1], [0,1,0]])\n",
    "sharpened = cv2.filter2D(gray, -1, kernel)\n",
    "cv2.imshow('Original', gray)\n",
    "cv2.imshow('Sharpened', sharpened)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee388d-eeeb-4131-b273-cfcf3a36df24",
   "metadata": {},
   "source": [
    "Question 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc1192-1ae7-4dd4-b9a0-6563cf2123c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\")\n",
    "img = cv2.resize(img,(255, 255))\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# Apply the Laplacian filter with a positive center coefficient\n",
    "laplacian_kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "sharpened_img = cv2.filter2D(gray_img, -1, laplacian_kernel)\n",
    "sharpened_img = cv2.cvtColor(sharpened_img, cv2.COLOR_GRAY2BGR)\n",
    "cv2.imshow('Original Image', img)\n",
    "cv2.imshow('Sharpened Image', sharpened_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4007261-a970-4f88-8a9b-4856217c9a9f",
   "metadata": {},
   "source": [
    "Question 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ebccc-bfe9-4cb1-9a68-7ba30ea5c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "laplacian_kernel = np.array([[0, 1, 0],\n",
    " [1, -4, 1],\n",
    " [0, 1, 0]])\n",
    "laplacian = cv2.filter2D(gray, -1, laplacian_kernel)\n",
    "sharpened = cv2.add(gray, laplacian)\n",
    "cv2.imshow('Original Image', gray)\n",
    "cv2.imshow('Sharpened Image', sharpened)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427d11f-4ebe-4bd7-8146-38831aa964e1",
   "metadata": {},
   "source": [
    "Question 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b74d61-56aa-4bd7-a580-56ea53176b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def high_boost_filter(image, kernel_size, k):\n",
    "    # Apply Gaussian smoothing\n",
    "    blurred = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n",
    "    \n",
    "    # Compute the high-boost filter mask\n",
    "    mask = image - blurred\n",
    "    \n",
    "    # Add the mask to the original image with amplification factor k\n",
    "    sharpened = image + k * mask\n",
    "    \n",
    "    # Clip pixel values to ensure they remain within [0, 255]\n",
    "    sharpened = np.clip(sharpened, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return sharpened\n",
    "\n",
    "# Read the image\n",
    "image = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\")\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply high-boost filtering\n",
    "sharpened_image = high_boost_filter(gray_image, kernel_size=5, k=1.5)\n",
    "\n",
    "# Display the original and sharpened images\n",
    "cv2.imshow('Original Image', gray_image)\n",
    "cv2.imshow('Sharpened Image', sharpened_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e41627-6cc5-4edd-a64e-d60a3d9997f7",
   "metadata": {},
   "source": [
    "Question 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12961bcc-e293-43ee-9aff-b53d8e92739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read the image\n",
    "image = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\")\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Compute gradients in both x and y directions using Sobel operator\n",
    "grad_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "grad_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "# Combine the gradients to get the magnitude\n",
    "gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "\n",
    "# Scale the gradient magnitude to [0, 255]\n",
    "gradient_magnitude_scaled = cv2.normalize(gradient_magnitude, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "\n",
    "# Apply the gradient mask to the original image\n",
    "sharpened_image = cv2.addWeighted(gray_image, 1.5, gradient_magnitude_scaled, -1.5, 0)\n",
    "\n",
    "# Display the original and sharpened images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Sharpened Image', sharpened_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258a6eca-5553-4a38-9b85-af7eb2457b05",
   "metadata": {},
   "source": [
    "Question 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa972ad4-bc2e-499d-91ab-e73c68fddd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Read the image and watermark\n",
    "img = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample.png\")\n",
    "wm = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\download.jpeg\")\n",
    "\n",
    "# Resize the watermark to fit on the image\n",
    "h_img, w_img = img.shape[:2]\n",
    "wm = cv2.resize(wm, (int(w_img * 0.2), int(h_img * 0.2)))  # Adjust the scaling factor as needed\n",
    "\n",
    "# Get dimensions of watermark\n",
    "h_wm, w_wm = wm.shape[:2]\n",
    "\n",
    "# Calculate positions for watermark placement\n",
    "top_y = h_img - h_wm - 10  # 10 pixels from the bottom\n",
    "left_x = w_img - w_wm - 10  # 10 pixels from the right\n",
    "\n",
    "# Insert watermark into image\n",
    "roi = img[top_y:top_y+h_wm, left_x:left_x+w_wm]\n",
    "result = cv2.addWeighted(roi, 1, wm, 0.3, 0)\n",
    "img[top_y:top_y+h_wm, left_x:left_x+w_wm] = result\n",
    "\n",
    "# Display the watermarked image\n",
    "cv2.imshow(\"Watermarked Image\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddbca43-2938-4952-83e7-ff0c511d2af6",
   "metadata": {},
   "source": [
    "Question 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e63c17e1-4a6c-4dae-9a5d-8f3fc9d88d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 225, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample.png\")\n",
    "img2 = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\")\n",
    "print(image.shape) # Print image shape\n",
    "imageCopy = image.copy()\n",
    "cv2.circle(imageCopy, (10, 100), 30, (255, 0, 0), -1)\n",
    "cropped_image = image[100:200, 100:200]\n",
    "cv2.imshow(\"cropped\", cropped_image)\n",
    "dst = cv2.addWeighted(image, 0.5, img2, 0.7, 0)\n",
    "cv2.imshow('Blended Image',dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d73e0-2122-411a-98bd-8ab39e50e4bd",
   "metadata": {},
   "source": [
    "Question 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce3498f-3e10-457e-80d6-4ca56f0ad26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\", \n",
    "cv2.IMREAD_GRAYSCALE)\n",
    "dx = cv2.Sobel(img, cv2.CV_64F, 1, 0)\n",
    "dy = cv2.Sobel(img, cv2.CV_64F, 0, 1)\n",
    "edges = cv2.magnitude(dx, dy)\n",
    "thresh = 100\n",
    "edges[edges < thresh] = 0\n",
    "edges[edges >= thresh] = 255\n",
    "cv2.imshow(\"Edges\", edges)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc77ab-fd29-46ec-b44d-2c7714b22455",
   "metadata": {},
   "source": [
    "Question 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31470b6-e32f-40fd-8436-79a8fcecf904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\", cv2.IMREAD_GRAYSCALE)\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "erosion = cv2.erode(img, kernel, iterations=1)\n",
    "cv2.imshow(\"Original\", img)\n",
    "cv2.imshow(\"Erosion\", erosion)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9fa8d6-21fb-45a7-9776-a78acc79e7c3",
   "metadata": {},
   "source": [
    "Question 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a02296-269c-4156-8cc9-129c4b488880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\", cv2.IMREAD_GRAYSCALE)\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "dilation = cv2.dilate(img, kernel, iterations=1)\n",
    "cv2.imshow(\"Original\", img)\n",
    "cv2.imshow(\"Dilation\", dilation)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24227578-65d4-4c05-9102-34d8c41bf201",
   "metadata": {},
   "source": [
    "Question 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3a618fc-0668-4cd1-9599-130bc7243825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\Sample1.jpeg\", cv2.IMREAD_GRAYSCALE)\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "cv2.imshow(\"Original\", img)\n",
    "cv2.imshow(\"opening\", opening)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6852f2-c978-47a5-a1ae-7ebd4f7d2aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0e12708-dec8-43c7-860a-0ccba7fd7dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(r\"C:\\Users\\ashiv\\OneDrive\\Desktop\\open cv\\car_drift_racing (720p).mp4\")\n",
    "\n",
    "# Read all frames into a list\n",
    "frames = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frames.append(frame)\n",
    "\n",
    "# Display frames in reverse order\n",
    "for frame in reversed(frames):\n",
    "    cv2.imshow('Video in Reverse', frame)\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdff33d-a47f-4a0c-8e45-f6a00e6960ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
